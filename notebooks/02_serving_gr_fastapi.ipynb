{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf9317-1aef-41ea-be8d-3c173d5e72fd",
   "metadata": {},
   "source": [
    "# 1. Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a30a29-febb-49ea-b024-e9d75ff50de8",
   "metadata": {},
   "source": [
    "## 1.1 Install a Virtual env with all dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45177-b6d8-4103-91ff-82bf44b5e824",
   "metadata": {},
   "source": [
    "### 1.1.1 UV Based Environment Creation\n",
    "- Running below cell  requires uv to be installed on your machine. \n",
    "- You can install from https://docs.astral.sh/uv/pip/environments/\n",
    "- If you dont want UV please use pip based install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8d064-c3fa-4696-9dcd-c04d781bdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "uv venv guarded_llm_env\n",
    "source guarded_llm_env/bin/activate\n",
    "uv pip install ipykernel nbconvert\n",
    "uv pip install guardrails-ai==0.6.3 --prerelease allow\n",
    "uv pip install fastapi uvicorn nest-asyncio\n",
    "python -m ipykernel install --user --name=guarded_llm_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0f9ff-8b56-46f2-94dc-3edf255fb5cf",
   "metadata": {},
   "source": [
    "### 1.1.2 PIP Based Environment Creation\n",
    " - Uncomment below a dn run if you do want to not use above uv base install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f947a-7fe6-4986-ade3-63290946b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python -m pip install --user virtualenv\n",
    "# python -m virtualenv guarded_llm_env\n",
    "# source guarded_llm_env/bin/activate\n",
    "# python -m pip install ipykernel nbconvert\n",
    "# python -m pip install guardrails-ai==0.6.3 \n",
    "# python -m pip install fastapi uvicorn nest-asyncio\n",
    "# python -m ipykernel install --user --name=guarded_llm_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8052ff-78dc-4292-a794-da78d27b9c22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Activate the Kernel\n",
    "- refresh the browser\n",
    "- activate the _guarded_llm_env_ kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a8e88-56d0-4321-91d8-bed483c49771",
   "metadata": {},
   "source": [
    "# 2. Simple LLM Chat_Completions Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51fe6e-e15e-4c0f-9f93-52632dc81fe4",
   "metadata": {},
   "source": [
    "## 2.1 Set up your LLM Provider and Authentication token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50993188-f44f-41c4-a0f1-2b954746ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLM_API_TOKEN\"] = \"sk-123\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a335c0d-c644-47c4-a421-0e6ee38e3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LLM_PROVIDER_BASE=\"https://api.openai.com/v1\"\n",
    "LLM_API_TOKEN=os.environ[\"LLM_API_TOKEN\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27489bdd-5936-4694-8dd9-76e7587f6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionsReq(BaseModel):\n",
    "    model: str\n",
    "    messages: List[Message]\n",
    "    max_tokens: Optional[int] = 100\n",
    "    stream: Optional[bool] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8336b2a-48de-46d9-9df7-193482a1be0e",
   "metadata": {},
   "source": [
    "## 2.2 Make an simple chat_completions endpoint \n",
    "- I am using litellm completion method here\n",
    "- Reference : https://docs.litellm.ai/docs/completion/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02ad63-6a6e-4495-91cb-5a43eb2d0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from typing import Dict, Any\n",
    "\n",
    "def call_llm(provider_base, provider_key, *args, **kwargs) -> str:\n",
    "    \"\"\"Calls an LLM using litellm.completion.\"\"\"\n",
    "    #some bug in litellm version\n",
    "    if \"msg_history\" in kwargs:\n",
    "        kwargs.pop(\"msg_history\")\n",
    "        \n",
    "    response = litellm.completion(\n",
    "        api_base=provider_base,\n",
    "        api_key=provider_key,\n",
    "        **kwargs\n",
    "    )\n",
    "    if \"stream\" in kwargs and kwargs[\"stream\"]:\n",
    "        for resp in response:\n",
    "            if resp.choices[0].delta.content:  # Some responses may not have content\n",
    "                chunk = resp.choices[0].delta.content\n",
    "                #print(chunk, end=\"\", flush=True)  # Print in real-time\n",
    "                yield chunk\n",
    "    else:\n",
    "         yield response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d404b-d9f5-4248-8e1d-6a1a413eb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import fastapi\n",
    "import uvicorn\n",
    "import threading\n",
    "from starlette.responses import StreamingResponse\n",
    "\n",
    "app = fastapi.FastAPI()\n",
    "\n",
    "@app.post(\"/chat_completions\")\n",
    "def chatcompletion(chat_req: ChatCompletionsReq):\n",
    "    chat_req_dict = chat_req.dict()\n",
    "    if chat_req.stream:\n",
    "        def stream_responses():\n",
    "            completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict)\n",
    "            for result in completion_outcome:\n",
    "                yield str(result) + \" \"\n",
    "\n",
    "        return StreamingResponse(stream_responses(), media_type=\"text/event-stream\")\n",
    "    else:\n",
    "        completion_outcome = completion_gg(chat_req)\n",
    "        if error:\n",
    "            return \" \".join(completion_outcome)\n",
    "        else:\n",
    "            res = \" \".join([v for v in completion_outcome])\n",
    "            return res\n",
    "\n",
    "# Function to run the server in a background thread\n",
    "def run():\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9000)\n",
    "\n",
    "# Start the FastAPI server in a separate thread\n",
    "server_thread = threading.Thread(target=run, daemon=True)\n",
    "server_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256338d-94b0-4950-82f7-2e28b0be19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:9000/chat_completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "     \"messages\":[\n",
    "         {\"role\": \"user\", \n",
    "          \"content\": \"Are python developers dumb idiotic and should they use rust\"}\n",
    "     ],\n",
    "    \"stream\":true,\n",
    "    \"max_tokens\":50,\n",
    "    \"model\": \"gpt-3.5-turbo\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b121e-b4fc-4bf4-98db-f8a6725d7add",
   "metadata": {},
   "source": [
    "# 3. Guarded LLM Chat_Completions Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2bee0-4d1e-400d-8f97-a094a8f5f24e",
   "metadata": {},
   "source": [
    "## 3.1 Install Guard from Guardrails HUB\n",
    " - Go to https://hub.guardrailsai.com/\n",
    " - Get Your token and configure it locally\n",
    " - Install your required guard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55825e7-0721-48ec-9e42-75b7ae82f5e1",
   "metadata": {},
   "source": [
    "### 3.1.1 Configure Hub Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265518d-d2e4-4f38-bb57-688912d39539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GR_TOKEN\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e33f5-3ed6-44ed-af60-36971dd74b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source guarded_llm_env/bin/activate\n",
    "guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709256b-3ec6-473e-b400-5bc5ca22171e",
   "metadata": {},
   "source": [
    "### 3.1.2 Install Guardrail From Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8083-f0bc-4e2d-831e-fc51cfdf2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source guarded_llm_env/bin/activate && guardrails hub install hub://guardrails/profanity_free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0052a-f6f2-4e83-94d6-76b026732d57",
   "metadata": {},
   "source": [
    "## 3.2 Call LLM with Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf2dd8-49e8-4068-a11c-09eb361fab10",
   "metadata": {},
   "source": [
    "### 3.2.1 Initialize Guardrail Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e88803-3a97-4603-acc8-47d71799bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "from guardrails.hub import ProfanityFree\n",
    "from guardrails import OnFailAction\n",
    "profanity_guard =  gd.Guard(name=\"Profanity\").use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1411d-6bee-4c83-ad84-1814c4417f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a New Schema to Support Guards\n",
    "class ChatCompletionsReqGuarded(BaseModel):\n",
    "    model: str\n",
    "    messages: List[Message]\n",
    "    max_tokens: Optional[int] = 100\n",
    "    stream: Optional[bool] = True\n",
    "    guard_to_apply: Optional[str] = None\n",
    "\n",
    "\n",
    "available_guards ={\"Profanity\":profanity_guard}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77041e43-588c-4b86-878c-1d2021739a59",
   "metadata": {},
   "source": [
    "### 3.2.2 Expose an Guarded chat_completions endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e632e-72b3-424e-9bb2-1484d4fb4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -> str:\n",
    "    \"\"\"Calls an LLM with Profanity Guard\"\"\"\n",
    "    if guard_to_apply:\n",
    "\n",
    "        #Validate Input Only\n",
    "        try:\n",
    "            for msg in chat_request[\"messages\"]:\n",
    "                guard_to_apply.validate(msg[\"content\"])\n",
    "        except Exception as e:\n",
    "            error_str = \"INPUT_GUARD_FAILED::\" + str(e)\n",
    "            yield error_str\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            #FIX ME SOME BUG HERE\n",
    "            llm_output_gen = profanity_guard(call_llm,\n",
    "                                            provider_base=LLM_PROVIDER_BASE, \n",
    "                                            provider_key=LLM_API_TOKEN, \n",
    "                                            **chat_request)\n",
    "            for validation_outcome in llm_output_gen:\n",
    "                if validation_outcome.validation_passed==True:\n",
    "                    yield validation_outcome.validated_output\n",
    "        except Exception as e:\n",
    "            error_str = \"OUTPUT_GUARD_FAILED::\" + str(e)\n",
    "            print(error_str)\n",
    "            yield error_str\n",
    "            #return \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,\n",
    "                                   provider_key=LLM_API_TOKEN,\n",
    "                                   **user_chat_request):\n",
    "            yield chunk_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7b328-cadd-4c77-a79d-7fc8b440edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import fastapi\n",
    "import uvicorn\n",
    "import threading\n",
    "from starlette.responses import StreamingResponse\n",
    "\n",
    "app_guarded = fastapi.FastAPI()\n",
    "\n",
    "@app_guarded.post(\"/ChatCompletionsReqGuarded\")\n",
    "def chatcompletion(chat_req: ChatCompletionsReqGuarded):\n",
    "    chat_req_dict = chat_req.dict()\n",
    "    guard_to_apply = available_guards[chat_req.guard_to_apply]\n",
    "    chat_req_dict.pop(\"guard_to_apply\")\n",
    "    if chat_req.stream:\n",
    "        def stream_responses():\n",
    "            completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, \n",
    "                                                  provider_key=LLM_API_TOKEN, \n",
    "                                                  chat_request=chat_req_dict, \n",
    "                                                  guard_to_apply=guard_to_apply)\n",
    "            for result in completion_outcome:\n",
    "                yield str(result) + \" \"\n",
    "\n",
    "        return StreamingResponse(stream_responses(), media_type=\"text/event-stream\")\n",
    "    else:\n",
    "        completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, \n",
    "                                                  provider_key=LLM_API_TOKEN, \n",
    "                                                  chat_request=chat_req_dict, \n",
    "                                                  guard_to_apply=guard_to_apply)\n",
    "        return completion_outcome#FIX THIS\n",
    "\n",
    "# Function to run the server in a background thread\n",
    "def run():\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app_guarded, \n",
    "                host=\"0.0.0.0\", \n",
    "                port=8000)\n",
    "\n",
    "# Start the FastAPI server in a separate thread\n",
    "server_thread = threading.Thread(target=run, daemon=True)\n",
    "server_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef453b-1233-43f1-b2ca-522d1a9e5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/ChatCompletionsReqGuarded' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "     \"messages\":[\n",
    "         {\"role\": \"user\", \n",
    "          \"content\": \"Are python developers dumb idiotic and should they use rust \"}\n",
    "     ],\n",
    "    \"stream\":true,\n",
    "    \"max_tokens\":50,\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"guard_to_apply\":\"Profanity\"\n",
    "\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382be097-c923-422a-85a3-8fad0becbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/ChatCompletionsReqGuarded' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "     \"messages\":[\n",
    "         {\"role\": \"user\", \n",
    "          \"content\": \"Complete the below sentence. he is in id**t \"}\n",
    "     ],\n",
    "    \"stream\":true,\n",
    "    \"max_tokens\":50,\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"guard_to_apply\":\"Profanity\"\n",
    "\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e386411-800c-47dd-8c87-0a01bb512383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guarded_llm_env",
   "language": "python",
   "name": "guarded_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
