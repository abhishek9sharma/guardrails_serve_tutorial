{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf9317-1aef-41ea-be8d-3c173d5e72fd",
   "metadata": {},
   "source": [
    "# 1. Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a30a29-febb-49ea-b024-e9d75ff50de8",
   "metadata": {},
   "source": [
    "## 1.1 Install a Virtual env with all dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45177-b6d8-4103-91ff-82bf44b5e824",
   "metadata": {},
   "source": [
    "### 1.1.1 UV Based Environment Creation\n",
    "- Running below cell  requires uv to be installed on your machine. \n",
    "- You can install from https://docs.astral.sh/uv/pip/environments/\n",
    "- If you dont want UV please use pip based install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e0efa-9178-4c9d-9968-6e6e34968e6d",
   "metadata": {},
   "source": [
    "%%bash\n",
    "uv venv guarded_llm_env\n",
    "source guarded_llm_env/bin/activate\n",
    "uv pip install ipykernel nbconvert\n",
    "uv pip install guardrails-ai==0.6.3 --prerelease allow\n",
    "uv pip install fastapi uvicorn nest-asyncio\n",
    "python -m ipykernel install --user --name=guarded_llm_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0f9ff-8b56-46f2-94dc-3edf255fb5cf",
   "metadata": {},
   "source": [
    "### 1.1.2 PIP Based Environment Creation\n",
    " - Uncomment below a dn run if you do want to not use above uv base install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f947a-7fe6-4986-ade3-63290946b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python -m pip install --user virtualenv\n",
    "# python -m virtualenv guarded_llm_env\n",
    "# source guarded_llm_env/bin/activate\n",
    "# python -m pip install ipykernel nbconvert\n",
    "# python -m pip install guardrails-ai==0.6.3 \n",
    "# python -m pip install fastapi uvicorn nest-asyncio\n",
    "# python -m ipykernel install --user --name=guarded_llm_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8052ff-78dc-4292-a794-da78d27b9c22",
   "metadata": {},
   "source": [
    "## 1.2 Activate the Kernel\n",
    "- refresh the browser\n",
    "- activate the _guarded_llm_env_ kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a8e88-56d0-4321-91d8-bed483c49771",
   "metadata": {},
   "source": [
    "# 2. Normal LLM Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51fe6e-e15e-4c0f-9f93-52632dc81fe4",
   "metadata": {},
   "source": [
    "## 2.1 Set up your LLM Provider and Authentication token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50993188-f44f-41c4-a0f1-2b954746ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"LLM_API_TOKEN\"] = \"sk-123\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a335c0d-c644-47c4-a421-0e6ee38e3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LLM_PROVIDER_BASE=\"https://api.openai.com/v1\"\n",
    "LLM_API_TOKEN=os.environ[\"LLM_API_TOKEN\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a1a3e-7f04-4f57-b45d-e46b749b4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_chat_request = {\n",
    "     \"messages\":[\n",
    "         {\"role\": \"user\", \n",
    "          \"content\": \"Are python developers dumb idiotic and should they use rust\"}\n",
    "     ],\n",
    "    \"stream\":True,\n",
    "    \"max_tokens\":50,\n",
    "    \"model\": \"gpt-3.5-turbo\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8336b2a-48de-46d9-9df7-193482a1be0e",
   "metadata": {},
   "source": [
    "## 2.2 Make an normal LLM Call\n",
    "- I am using litellm completion method here\n",
    "- Reference : https://docs.litellm.ai/docs/completion/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02ad63-6a6e-4495-91cb-5a43eb2d0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from typing import Dict, Any\n",
    "\n",
    "def call_llm(provider_base, provider_key, *args, **kwargs) -> str:\n",
    "    \"\"\"Calls an LLM using litellm.completion.\"\"\"\n",
    "\n",
    "    #some bug in litellm version\n",
    "    if \"msg_history\" in kwargs:\n",
    "        kwargs.pop(\"msg_history\")\n",
    "        \n",
    "    response = litellm.completion(\n",
    "        api_base=provider_base,\n",
    "        api_key=provider_key,\n",
    "        **kwargs\n",
    "    )\n",
    "    if \"stream\" in kwargs and kwargs[\"stream\"]:\n",
    "        for resp in response:\n",
    "            if resp.choices[0].delta.content:  # Some responses may not have content\n",
    "                chunk = resp.choices[0].delta.content\n",
    "                #print(chunk, end=\"\", flush=True)  # Print in real-time\n",
    "                yield chunk\n",
    "    else:\n",
    "         yield response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256338d-94b0-4950-82f7-2e28b0be19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,\n",
    "                            provider_key=LLM_API_TOKEN, \n",
    "                            **user_chat_request):\n",
    "    print(chunk_resp, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b121e-b4fc-4bf4-98db-f8a6725d7add",
   "metadata": {},
   "source": [
    "# 3. Guarded LLM Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2bee0-4d1e-400d-8f97-a094a8f5f24e",
   "metadata": {},
   "source": [
    "## 3.1 Install Guard from Guardrails HUB\n",
    " - Go to https://hub.guardrailsai.com/\n",
    " - Get Your token and configure it locally\n",
    " - Install your required guard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55825e7-0721-48ec-9e42-75b7ae82f5e1",
   "metadata": {},
   "source": [
    "### 3.1.1 Configure Hub Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265518d-d2e4-4f38-bb57-688912d39539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"GR_TOKEN\"]=\"GRTOKENHERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e33f5-3ed6-44ed-af60-36971dd74b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source guarded_llm_env/bin/activate\n",
    "guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709256b-3ec6-473e-b400-5bc5ca22171e",
   "metadata": {},
   "source": [
    "### 3.1.2 Install Guardrail From Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8083-f0bc-4e2d-831e-fc51cfdf2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source guarded_llm_env/bin/activate && guardrails hub install hub://guardrails/profanity_free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0052a-f6f2-4e83-94d6-76b026732d57",
   "metadata": {},
   "source": [
    "## 3.2 Call LLM with Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf2dd8-49e8-4068-a11c-09eb361fab10",
   "metadata": {},
   "source": [
    "### 3.2.1 Initialize Guardrail Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e88803-3a97-4603-acc8-47d71799bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "from guardrails.hub import ProfanityFree\n",
    "from guardrails import OnFailAction\n",
    "profanity_guard =  gd.Guard(name=\"Profanity\").use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48877cca-4db5-4da7-a6bb-b6d3c470f513",
   "metadata": {},
   "source": [
    "### 3.2.2 Apply GuardRails on Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa16c6b-9cc4-4616-adbc-ca3253cc1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_guard_input(provider_base, provider_key, chat_request: Dict[str, Any], guard_to_apply=None) -> str:\n",
    "    \"\"\"Calls an LLM with Profanity Guard\"\"\"\n",
    "    if guard_to_apply:\n",
    "        #Validate Input Only\n",
    "        try:\n",
    "            for msg in chat_request[\"messages\"]:\n",
    "                guard_to_apply.validate(msg[\"content\"])\n",
    "        except Exception as e:\n",
    "            error_str = \"INPUT_GUARD_FAILED::\" + str(e)\n",
    "            yield error_str\n",
    "\n",
    "            \n",
    "    else:\n",
    "        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,\n",
    "                                   provider_key=LLM_API_TOKEN,\n",
    "                                   **user_chat_request):\n",
    "            yield chunk_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d1793-5870-47aa-a6bf-8ff30856280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call with Input Guards\n",
    "for chunk_resp in call_llm_guard_input(provider_base=LLM_PROVIDER_BASE, \n",
    "                                       provider_key=LLM_API_TOKEN, \n",
    "                                       chat_request=user_chat_request, \n",
    "                                       guard_to_apply=profanity_guard):\n",
    "    print(chunk_resp, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77041e43-588c-4b86-878c-1d2021739a59",
   "metadata": {},
   "source": [
    "### 3.2.3 Apply GuardRails on Inputs plus Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881ae6e-3254-47a9-9037-aa488d4d9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_guard_output(provider_base, provider_key, chat_request: Dict[str, Any], guard_to_apply=None) -> str:\n",
    "    \"\"\"Calls an LLM with Profanity Guard\"\"\"\n",
    "    if guard_to_apply:\n",
    "        \n",
    "        try:\n",
    "            llm_output_gen =profanity_guard(call_llm,provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, **chat_request)\n",
    "            for validation_outcome in llm_output_gen:\n",
    "                if validation_outcome.validation_passed==True:\n",
    "                    yield validation_outcome.validated_output\n",
    "        except Exception as e:\n",
    "            error_str = \"OUTPUT_GUARD_FAILED::\" + str(e)\n",
    "            yield error_str        \n",
    "            \n",
    "    else:\n",
    "        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,\n",
    "                                   provider_key=LLM_API_TOKEN, \n",
    "                                   **user_chat_request):\n",
    "            yield chunk_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e15a9f-53b9-4dde-8491-306fedec447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call LLM with OUTPUT  Guards\n",
    "for chunk_resp in call_llm_guard_output(provider_base=LLM_PROVIDER_BASE,\n",
    "                                        provider_key=LLM_API_TOKEN,\n",
    "                                        chat_request=user_chat_request,\n",
    "                                        guard_to_apply=profanity_guard):\n",
    "    print(chunk_resp, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e632e-72b3-424e-9bb2-1484d4fb4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_guarded(provider_base, provider_key, chat_request: Dict[str, Any], guard_to_apply=None) -> str:\n",
    "    \"\"\"Calls an LLM with Profanity Guard\"\"\"\n",
    "    if guard_to_apply:\n",
    "\n",
    "        #Validate Input Only\n",
    "        try:\n",
    "            for msg in chat_request[\"messages\"]:\n",
    "                guard_to_apply.validate(msg[\"content\"])\n",
    "        except Exception as e:\n",
    "            error_str = \"INPUT_GUARD_FAILED::\" + str(e)\n",
    "            yield error_str\n",
    "\n",
    "        try:\n",
    "            llm_output_gen =profanity_guard(call_llm,\n",
    "                                            provider_base=LLM_PROVIDER_BASE, \n",
    "                                            provider_key=LLM_API_TOKEN, \n",
    "                                            **chat_request)\n",
    "            for validation_outcome in llm_output_gen:\n",
    "                if validation_outcome.validation_passed==True:\n",
    "                    yield validation_outcome.validated_output\n",
    "        except Exception as e:\n",
    "            error_str = \"OUTPUT_GUARD_FAILED::\" + str(e)\n",
    "            yield error_str        \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,\n",
    "                                   provider_key=LLM_API_TOKEN,\n",
    "                                   **user_chat_request):\n",
    "            yield chunk_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ca646-a50f-491e-8ab5-345a22a6a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_chat_request_new = {\n",
    "     \"messages\":[\n",
    "         {\"role\": \"user\", \n",
    "          \"content\": \"Complete the below sentence. he is in id**t \"}\n",
    "     ],\n",
    "    \"stream\":True,\n",
    "    \"max_tokens\":50,\n",
    "    \"model\": \"gpt-3.5-turbo\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cc7d8-721e-43e6-a904-fb991fe0fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call LLM with OUTPUT  Guards\n",
    "for chunk_resp in call_llm_guarded(provider_base=LLM_PROVIDER_BASE, \n",
    "                                        provider_key=LLM_API_TOKEN, \n",
    "                                        chat_request=user_chat_request_new,\n",
    "                                        guard_to_apply=profanity_guard):\n",
    "    print(chunk_resp, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7b328-cadd-4c77-a79d-7fc8b440edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guarded_llm_env",
   "language": "python",
   "name": "guarded_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
